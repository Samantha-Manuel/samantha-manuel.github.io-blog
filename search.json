[
  {
    "objectID": "blogabout.html",
    "href": "blogabout.html",
    "title": "About",
    "section": "",
    "text": "My name is Samantha Manuel, and I am a second year PhD student in the Criminology program here at UT Dallas. I hope to help close the gap in research regarding the effects of child maltreatment on juvenile delinquency and adult criminality, and most importantly, help find potential methods to mitigate and prevent these outcomes. I recently presented my thesis, “‘I’m Still Here, I’m Still Alive’: Resilience Among Survivors of Child Maltreatment”. I presented my thesis at the Academy of Criminal Justice Sciences 59th Conference in Las Vegas, Nevada, on March 19th, 2022. This thesis illustrated how child abuse affects one’s ability to resist criminality and deviance in later adulthood. This study intended to provide a better understanding on an individual level as to why victims of childhood maltreatment choose to promote resilience and quality of life instead of criminal or antisocial behavior. By continuing with a PhD, I hope to further increase the criminological field’s understanding of why some individuals do not become criminal, and go against what is expected of them.\nMy academic interests tend to focus on juvenile delinquency and related topics. My personal belief is that the youth of society are the future, and it is imperative that scholars and policy makers alike emphasize the support and scaffolding of young community members. Once I earn my PhD, I hope to become a professor. In this position, I can help teach others the importance of research and policy development. Through my continued education here at UT Dallas, I hope to gain the skills and tools I need to become a better teacher and role model for my students. I aim to inspire the youth of these communities to dream big and to become productive members of society. In addition to teaching, professors are also expected to lead research studies. I hope to use my position to help further criminological research by applying theory to policy. In this way, I will be able to best use my position to advocate for programs and initiatives that can help improve our communities.\nIn future research, I would like to continue to investigate the causes and effects of juvenile delinquency, specifically in relation to childhood maltreatment. Furthermore, I intend to expand the recently developed concept of resiliency, which is of particular interest to me. It is inspiring to me that individuals who researchers have predicted to be deviant and criminal defy what is expected of them, and ultimately become dynamic and impactful members within their community. I also hope to close the research gap on child maltreatment, resiliency, and mental health related to minority populations, especially Asian Americans. As a future researcher, I would like to empirically assess potential policies that may positively influence these populations of individuals, and help them reduce recidivism in the long run. By obtaining my PhD degree here at UT Dallas, I will better equip myself with a vast background knowledge in my field, while also fine-tuning my teaching abilities. In this way, I hope to have as much positive influence as possible on scholars to come."
  },
  {
    "objectID": "posts/assign02/index.html",
    "href": "posts/assign02/index.html",
    "title": "EPPS 6356 Assignment 2",
    "section": "",
    "text": "1.Build a Quarto blog on your personal website.\nLink: https://samantha-manuel.github.io/samantha-manuel.github.io-blog/\n2.Run Paul Murrell’s RGraphics basic R programs (murrell01.R in GitHub).\na. Be sure to run line by line and note the changes.\nb. Pay attention to the comments and address the question if there is one.\ni. plot(pressure, pch=16) # Can you change pch?\nYes, you can change the pch. Changing the pch alters the points on the plotline of the graph. For example, the value 16 for the pch are circles. If the pch is changed to 15, the circles change to squares.\nii. points(x, y1, pch=16, cex=3) # Try different cex value?\nThe cex value changes the size of the points on the plot line. The larger the cex value, the larger the points on the plot line.\niii. axis(1, at=seq(0, 16, 4)) # What is the first number standing for?\nThis command generates the x axis on the chart. The x axis is labelled “Travel Time in seconds”. This means that the first number on the x axis, 0, signifies when the time measurement begins; it is just before the responses are being recorded.\niv. Generations of these Murrell charts are below.\n\n\n\n\n\nFigure 1: Pressure plot with the pch value changed from 16 to 15.\n\n\n\n\n\nFigure 2: Bird 131 Travel scatterplot with the cex value changed from 2 to 3, as well as the “background color” of the white circles changed to grey circles.\n\n\n\n\n\nFigure 3: Histogram of Y.\n\n\n\n\n\nFigure 4: Barplot.\n\n\n\n\n\nFigure 5: Boxplot of Vitamin C dose (mg) on tooth growth/length.\n\n\n\n\n\nFigure 6: Perspective plot.\n\n\n\n\n\nFigure 7: Pie chart of pie sales.\nv. Try these functions using another dataset. Be sure to work on the layout and margins.\nSee Part C below. These functions will be used on the Happy Planet Index (HPI) data set.\nc. Plotting functions using the Happy Planet Index (HPI) data set.\n\n# Setting up the new R environment, starting fresh, click run!\nrm(list=ls())\n\n# Setting up the working directory, click run!\nsetwd(\"~/Documents/Fall 2022/EPPS 6356/samantha-manuel.github.io\")\n\n# Turning on the packages required for HW2, click run!\nlibrary(Hmisc)\n\nLoading required package: lattice\n\n\nLoading required package: survival\n\n\nLoading required package: Formula\n\n\nLoading required package: ggplot2\n\n\n\nAttaching package: 'Hmisc'\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\nlibrary(xlsx)\nlibrary(tidyverse)\n\n── Attaching packages\n───────────────────────────────────────\ntidyverse 1.3.2 ──\n\n\n✓ tibble  3.1.6     ✓ dplyr   1.0.8\n✓ tidyr   1.2.0     ✓ stringr 1.4.0\n✓ readr   2.1.1     ✓ forcats 0.5.1\n✓ purrr   0.3.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\nx dplyr::filter()    masks stats::filter()\nx dplyr::lag()       masks stats::lag()\nx dplyr::src()       masks Hmisc::src()\nx dplyr::summarize() masks Hmisc::summarize()\n\nlibrary(maps)\n\n\nAttaching package: 'maps'\n\nThe following object is masked from 'package:purrr':\n\n    map\n\nlibrary(countrycode)\nlibrary(ggplot2)\n\n# Reading the file, click run!\nHPI <- read.xlsx(\"~/Documents/Fall 2022/EPPS 6356/samantha-manuel.github.io/happy-planet-index-2006-2020-public-data-set.xlsx\", sheetIndex = 2)\n\n#Remove first 7 rows and save as 'HPI2019', click run! \nHPI2019 <- HPI[-c(1:7),]\nview(HPI2019)\n\n#remove 3rd column ('NA..2') as it is not needed for further analysis, click run! \nHPI2019 <- HPI2019 %>%\n    select(-'NA..2')\n\n#Rename columns for simplicity, click run!\nHPI2019 <- HPI2019 %>%\n    rename(HPI_rank = 'NA.') %>%\n    rename(Country = 'X1..Rankings.for.all.countries..2006...2020') %>%\n    rename(ISO = 'NA..1') %>%\n    rename(Continent = 'NA..3') %>%\n    rename(Pop = 'NA..4') %>%\n    rename(Life_Exp = 'NA..5') %>%\n    rename(Wellbeing = 'NA..6') %>%\n    rename(Ecological_Footprint = 'NA..7') %>%\n    rename(HPI = 'NA..8') %>%\n    rename(Biocapacity = 'NA..9') %>%\n    rename(GDP_per_capita = 'NA..10')\n\n#Remove row with index number 8 so the data set starts with 'Costa Rica', click run! \nHPI2019 <- HPI2019[-1,]\n\n#Change to show a maximum of 15 digits, click run! \noptions(digits = 15)\n\n#Convert data type from character to numeric for selected columns, click run! \nHPI2019 <- HPI2019 %>%\n    mutate(HPI_rank = as.numeric(HPI_rank)) %>%\n    mutate(Pop = as.numeric(Pop)) %>%\n    mutate(Life_Exp = as.numeric(Life_Exp)) %>%\n    mutate(Wellbeing = as.numeric(Wellbeing)) %>%\n    mutate(Ecological_Footprint = as.numeric(Ecological_Footprint)) %>%\n    mutate(HPI = as.numeric(HPI)) %>%\n    mutate(Biocapacity = as.numeric(Biocapacity)) %>%\n    mutate(GDP_per_capita = as.numeric(GDP_per_capita))\n\nWarning in mask$eval_all_mutate(quo): NAs introduced by coercion\n\n#View cleaned data set, click run! \nstr(HPI2019)\n\n'data.frame':   152 obs. of  11 variables:\n $ HPI_rank            : num  1 2 3 4 5 6 7 8 9 10 ...\n $ Country             : chr  \"Costa Rica\" \"Vanuatu\" \"Colombia\" \"Switzerland\" ...\n $ ISO                 : chr  \"CRI\" \"VUT\" \"COL\" \"CHE\" ...\n $ Continent           : chr  \"1\" \"8\" \"1\" \"3\" ...\n $ Pop                 : num  5048 300 50339 8591 17374 ...\n $ Life_Exp            : num  80.3 70.5 77.3 83.8 77 78.5 74.5 74.3 75.3 77.9 ...\n $ Wellbeing           : num  7 6.96 6.35 7.69 5.81 ...\n $ Ecological_Footprint: num  2.65 1.62 1.9 4.14 1.51 ...\n $ HPI                 : num  62.1 60.4 60.2 60.1 58.8 ...\n $ Biocapacity         : num  1.56 1.56 1.56 1.56 1.56 1.56 1.56 1.56 1.56 1.56 ...\n $ GDP_per_capita      : num  20297 3153 14625 68391 11375 ...\n\nhead(HPI2019)\n\n   HPI_rank     Country ISO Continent       Pop Life_Exp        Wellbeing\n9         1  Costa Rica CRI         1  5047.561     80.3 6.99761867523193\n10        2     Vanuatu VUT         8   299.882     70.5 6.95562031699646\n11        3    Colombia COL         1 50339.443     77.3 6.35029792785645\n12        4 Switzerland CHE         3  8591.361     83.8 7.69422101974487\n13        5     Ecuador ECU         1 17373.657     77.0 5.80913114547729\n14        6      Panama PAN         1  4246.440     78.5 6.08595514297485\n   Ecological_Footprint              HPI Biocapacity    GDP_per_capita\n9      2.64852226555336 62.0575517701467        1.56 20296.82150273480\n10     1.61609366019406 60.3638758082503        1.56  3153.01516775842\n11     1.90475086511154 60.1651674984682        1.56 14624.97129653470\n12     4.14251611511677 60.1046503431779        1.56 68390.71298545389\n13     1.50707276829587 58.8312130560036        1.56 11375.33118432910\n14     2.09549713846328 57.9321698535130        1.56 31458.69262552130\n\nview(HPI2019)\n\n#View map of data 'data_map', click run!\nrequire(maps)\nrequire(countrycode)\n\ndata_map <- map_data(\"world\")\nview(data_map)\n\n#Consulting the countrycode documentation for details, click run! \n?countrycode\n\n#Create a new column in data_map called 'ISO' to match the ISO column in the HPI2019 table, click run! \ndata_map$ISO = countrycode(data_map$region, origin=\"country.name\", destination = 'iso3c')\n\nWarning in countrycode_convert(sourcevar = sourcevar, origin = origin, destination = dest, : Some values were not matched unambiguously: Ascension Island, Azores, Barbuda, Bonaire, Canary Islands, Chagos Archipelago, Grenadines, Heard Island, Kosovo, Madeira Islands, Micronesia, Saba, Saint Martin, Siachen Glacier, Sint Eustatius, Virgin Islands\n\n#View updated data_map, click run! \nview(data_map)\n\n#Merge HPI2019 with data_map to create a data set which will be used to plot HPI in the world map, click run! \nmergedHPI2019 <- full_join(data_map, HPI2019, by=\"ISO\")\n\n#View merged data set, click run! \nview(mergedHPI2019)\n\n#Generate the world map chart, click run! \nggplot(mergedHPI2019, aes(x = long, y = lat, group = group, fill = HPI)) + geom_polygon() + scale_fill_viridis_c() +\n    labs(title = \"The State of Global Happiness in 2019\", subtitle = \"Based on the Happy Planet Index Score\")\n\n\n\n#Generate a barplot of the effect of GDP on HPI, click run! \nbarplot(table(mergedHPI2019$HPI,mergedHPI2019$GDP_per_capita), beside=TRUE, main= \"The Effect of GDP on HPI\", xlab= \"GDP\", ylab= \"HPI\")"
  },
  {
    "objectID": "posts/assign03/index.html",
    "href": "posts/assign03/index.html",
    "title": "EPPS 6356 Assignment 3",
    "section": "",
    "text": "1.Run anscombe01.R (in classGitHub)\na. Compare the regression models.\n\n\n\n\n\nFigure 1: Regression plot of X1 and Y1.\nAs seen in Figure 1, a regression plot is appropriate for data visualization and analysis. The plot points generally fit the regression line. However, because the plot points are very spaced out, the R^2 value may be very small, as the regression line does not fit plot points very well.\n\n\n\n\n\nFigure 2: Regression plot of X2 and Y2.\nAs seen in Figure 2, a regression plot is most likely not the best tool for data analysis. This is because all of the plot points resemble a parabolic function. For this reason, a different or supplementary data visualization/analysis tools may be required.\n\n\n\n\n\nFigure 3: Regression plot of X3 and Y3.\nFigure 3 is similar to Figure 1, in that a regression plot is appropriate for data visualization and analysis. The plot points generally fit the regression line, except for one outlier point at x=13. It may be beneficial to take this outlier out, and therefore visualize how much better the regression line would fit the data.\n\n\n\n\n\nFigure 4: Regression plot of X4 and Y4.\nLike Figure 2, a regression plot is most likely not the best tool for data analysis for Figure 4. This is because all of the plot points except for one are all at x=8. For this reason, different or supplementary data visualization/analysis tools are required.\nb. Compare different ways to create the plots (e.g. changing colors, line types, plot characters).\n\n\n\n\n\nFigure 5: Aggregate regression plots from X1-X4 and Y1-Y4.\nAs seen in Figure 5, all four regression plots were generated as a composite chart by using the code to \"plot for a loop\". This composite/aggregate chart also was about to generate a title to describe the plots. This code was also able to add further design components such as regression line color, as well as plot point size, color, and shape.\n2.Can you finetune the charts without using other packages (consult RGraphics by Murrell).\n\n\n\n\n\nFigure 6: Finetuned aggregate regression plots for X1-X4 and Y1-Y4.\nAbove is Figure 6, which is the \"finetuned\" version of the regression plots for X1-X4 and Y1-Y4, using the RGraphics by Murrell. I changed the plot points to be smaller, so that the reader can more effectively read the charts and distinguish between the plot points. This is especially helpful for Plot 4, where many of the plot points are stacked on top of each other. Making the plot point sizes smaller also allows the reader to be able see the distancebetween plot points easier, as well. I also changed the color of the plot points and regression line to colors that are easier on the eye and less distracting.\n3.How about with ggplot2? (use tidyverse package)\nSee the code below to see how the ggplot2 function (attached to the tidyverse package) can be used to efficiently generate the four regression plots for Anscombe's (1973) Quartlet. The generated chart is also seen below in Figure 7. This aggregate/composite plot was produced by using only two lines of code, instead of 13+ lines of code the traditional way without the ggplot2 function within the tidyverse package.\n\n\n\n\n\nFigure 7: Aggregate regression plots from X1-X4 and Y1-Y4 generated using the ggplot2 function.\n\n## Anscombe (1973) Quartlet\n\ndata(anscombe)  # Load Anscombe's data\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n## Simple version\nplot(anscombe$x1,anscombe$y1)\n\n\n\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n# Create four model objects\nlm1 <- lm(y1 ~ x1, data=anscombe)\nsummary(lm1)\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\nlm2 <- lm(y2 ~ x2, data=anscombe)\nsummary(lm2)\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\nlm3 <- lm(y3 ~ x3, data=anscombe)\nsummary(lm3)\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\nlm4 <- lm(y4 ~ x4, data=anscombe)\nsummary(lm4)\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\nplot.new\n\nfunction () \n{\n    for (fun in getHook(\"before.plot.new\")) {\n        if (is.character(fun)) \n            fun <- get(fun)\n        try(fun())\n    }\n    .External2(C_plot_new)\n    grDevices:::recordPalette()\n    for (fun in getHook(\"plot.new\")) {\n        if (is.character(fun)) \n            fun <- get(fun)\n        try(fun())\n    }\n    invisible()\n}\n<bytecode: 0x7fcc40a494e0>\n<environment: namespace:graphics>\n\nplot(y1 ~ x1, data=anscombe) + abline(lm(y1 ~ x1, data=anscombe))\n\n\n\n\ninteger(0)\n\nplot(y2 ~ x2, data=anscombe) + abline(lm(y2 ~ x2, data=anscombe))\n\n\n\n\ninteger(0)\n\nplot(y3 ~ x3, data=anscombe) + abline(lm(y3 ~ x3, data=anscombe))\n\n\n\n\ninteger(0)\n\nplot(y4 ~ x4, data=anscombe) + abline(lm(y4 ~ x4, data=anscombe))\n\n\n\n\ninteger(0)\n\n## Fancy version (per help file)\n\nff <- y ~ x\nmods <- setNames(as.list(1:4), paste0(\"lm\", 1:4))\n\n# Plot using for loop\nfor(i in 1:4) {\n  ff[2:3] <- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  ## or   ff[[2]] <- as.name(paste0(\"y\", i))\n  ##      ff[[3]] <- as.name(paste0(\"x\", i))\n  mods[[i]] <- lmi <- lm(ff, data = anscombe)\n  print(anova(lmi))\n}\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(>F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(>F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y3\n          Df Sum Sq Mean Sq F value   Pr(>F)   \nx3         1 27.470 27.4700  17.972 0.002176 **\nResiduals  9 13.756  1.5285                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y4\n          Df Sum Sq Mean Sq F value   Pr(>F)   \nx4         1 27.490 27.4900  18.003 0.002165 **\nResiduals  9 13.742  1.5269                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsapply(mods, coef)  # Note the use of this function\n\n                  lm1      lm2       lm3       lm4\n(Intercept) 3.0000909 3.000909 3.0024545 3.0017273\nx1          0.5000909 0.500000 0.4997273 0.4999091\n\nlapply(mods, function(fm) coef(summary(fm)))\n\n$lm1\n             Estimate Std. Error  t value    Pr(>|t|)\n(Intercept) 3.0000909  1.1247468 2.667348 0.025734051\nx1          0.5000909  0.1179055 4.241455 0.002169629\n\n$lm2\n            Estimate Std. Error  t value    Pr(>|t|)\n(Intercept) 3.000909  1.1253024 2.666758 0.025758941\nx2          0.500000  0.1179637 4.238590 0.002178816\n\n$lm3\n             Estimate Std. Error  t value    Pr(>|t|)\n(Intercept) 3.0024545  1.1244812 2.670080 0.025619109\nx3          0.4997273  0.1178777 4.239372 0.002176305\n\n$lm4\n             Estimate Std. Error  t value    Pr(>|t|)\n(Intercept) 3.0017273  1.1239211 2.670763 0.025590425\nx4          0.4999091  0.1178189 4.243028 0.002164602\n\n# Preparing for the plots\nop <- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))\n\n# Plot charts using for loop\nfor(i in 1:4) {\n  ff[2:3] <- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"darkorchid4\", pch = 23, bg = \"mediumpurple\", \n       cex = 0.75, xlim = c(3, 19), ylim = c(3, 13))\n  abline(mods[[i]], col = \"steelblue\")\n}\nmtext(\"Anscombe's 4 Regression Data Sets\", outer = TRUE, cex = 1.5)\n\n\n\npar(op)\n\n\n## 3.How about with ggplot2? (use tidyverse package)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✓ ggplot2 3.3.6     ✓ purrr   0.3.4\n✓ tibble  3.1.6     ✓ dplyr   1.0.8\n✓ tidyr   1.2.0     ✓ stringr 1.4.0\n✓ readr   2.1.1     ✓ forcats 0.5.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(dplyr)\nlibrary(broom)\n\ntidy_anscombe <- anscombe %>%\n  pivot_longer(cols = everything(),\n               names_to = c(\".value\", \"set\"),\n               names_pattern = \"(.)(.)\")\n\ntidy_anscombe\n\n# A tibble: 44 × 3\n   set       x     y\n   <chr> <dbl> <dbl>\n 1 1        10  8.04\n 2 2        10  9.14\n 3 3        10  7.46\n 4 4         8  6.58\n 5 1         8  6.95\n 6 2         8  8.14\n 7 3         8  6.77\n 8 4         8  5.76\n 9 1        13  7.58\n10 2        13  8.74\n# … with 34 more rows\n\nggplot(tidy_anscombe,\n       aes(x = x,\n           y = y)) +\n  geom_point() + \n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n#> `geom_smooth()` using formula 'y ~ x'"
  },
  {
    "objectID": "posts/edwardtufte/index.html",
    "href": "posts/edwardtufte/index.html",
    "title": "Edward Tufte (2016): ‘The Future of Data Analysis’ Review",
    "section": "",
    "text": "Link to the Video- Edward Tufte (2016): The Future of Data Analysis\nWe were asked to watch the Microsoft Machine Learning & Data Science Summit 2016 Keynote Session by Dr Edward Tufte titled “The Future of Data Analysis”. The video is linked above. Below is my review and reflection on the video.\nI think that the “R” Community Lead, David Smith, had a very insightful comment. He said, “data is really more than just numbers; data is something we use to tell a story, and that we communicate that story to another person, and being able to communicate that story effectively is such an important part of what we do”. This really resonates with me, because it makes data science and data analyses so much more three-dimensional and robust. I had the misconception in how I perceive data collection and visualization as something stagnant and two-dimensional. However, this is not the case. Data visualization and analysis is meant to be something that can describe, explain, and predict phenomena that we observe in nature and society. It is such an essential skill to have as a scholar, critical thinker, and social scientist.\nAccording to Edward Tufte, data analysis is about “turning information into conclusions, analytical thinking is about assessing and evaluating relationship between information and conclusions”. The purpose of data visualization or data display is “assist reasoning about its content”. What does this mean? To me, data display is meant to help further explain or demonstrate the relationship between “information and conclusions”, such as causal mechanisms and models that attempt to describe, explain, and predict phenomena that we observe in nature and society.\nTufte asserts that the crisis in data analysis is that most published studies are false. He reiterates that the purpose of data analysis is to use empirical information to learn about the world, to describe and explain something, the find causes and effects, to advance our understanding, and to get it right; to learn and tell the truth”. However, Tufte refers to an article by John P. A. Ioannidis, which claims that upwards of 35 percent of published research findings are false due to the study power and bias. Tufte even refers to Lazer and colleagues (2014) article on the ‘Google Flu’ regarding the ‘Traps in Big Data Analysis’, i.e. the tendency of data analysts to overfit their data. The most concerning issue (to me) is that not only are original studies’ data analyses and conclusions unable to be replicated, but the replication studies are not able to be replicated, either. This is an alarming issue that questions the validity of data analyses in empirical studies.\nAccording to Tufte, human science is not rocket science; it is harder than rocket science. This is so true! Human beings and human behaviors are so dynamic and continuously changing. Therefore, data analysis within the social science field needs to be continuously advanced and developed, as well. So, what is the future of data analysis? Tufte asserts that the future of data analysis is to “take seriously the distinction between studies that are confirmatory of an idea that have not been hacked or worked over versus exploratory detective work”. He explains that this means that we must “not create findings out of the content, but out of the analysis”. We must ensure the quality of our data analysis within social science by remaining conscious, prudent, and adaptive. This way, we can produce data analysis and conclusions to the best of our abilities."
  },
  {
    "objectID": "posts/googleflu_wickham/index.html",
    "href": "posts/googleflu_wickham/index.html",
    "title": "‘Google Flu’ and Hadley Wickham’s EMBL Review",
    "section": "",
    "text": "Part 1: “The Parable of Google Flu: Traps in Big Data Analysis”\n\n\n\n\n\nLazer, David, Kennedy, Ryan, King, Gary, and Vespignani, Alessandro, 2014. The Parable of Google Flu: Traps in Big Data Analysis. Science, [online] 343(6176), pp.1203-1205. Available at: <https://www.science.org/doi/full/10.1126/science.1248506> [Accessed 27 September 2022].\nHere is the link to the article: https://www.science.org/doi/full/10.1126/science.1248506\nCritique on:\ni.Big data analytics pitfall\nAccording to the authors (2014), ‘Big data hubris’ is the “often implicit assumption that big data are a substitute for, rather than a supplement to, traditional data collection and analysis”. This relates to the issue that quantity of data does not mean quality. Big data analytics fall to the concerns of measurement and construct validity issues, as well as reliability and dependency difficulties. This is essentially referring to the instruments used to measure the observations and data are not the best fit for producing the most valid and reliable data analysis and subsequent conclusion. Furthermore, Google heavily relies on algorithms to estimate future flu trends. This can be seen in the figure below. GFT overestimates the prevalence of flu during flu seasons, such as the 2012-2013 season seen in the figure. This is viewed as Google’s most common error, and as resulted in a panic instigated by the media (Lazer et al. 2014).\n\n\n\n\n\nii.Overfitting and overparameterization.\nAccording to Lazer and colleagues (2014), the Google Flu Trends (GFT) of 2013 have been “persistently overestimating flu prevalence for a much longer time” than just nine years ago. This is primarily due to errors associated with not being randomly distributed; this has been an issue with Google’s flu data analysis since even the 2011-2012 flu season (Lazer et al. 2014). Errors included using “last week’s errors [to] predict this week’s errors (temporal autocorrelation), and the direction and magnitude of error varies with the time of year (seasonality)” (Lazer et al. 2014). In other words, the GFT avoids the traditional use of statistical methods by overlooking pertinent information and data. This refers back to Google’s use of algorithms and computer programs to estimate flu trends instead of depending on actual data collection. The authors (2014) conclude that although big data offer “enormous possibilities for understanding human interactions at a societal scale”, it is also important to consider “small data”, as it provides supplemental perspective to data analyses.\nPart 2: Hadley Wickham’s (2019) presentation of “Data visualization and Data Science”\n\n\n\n\n\nLink to the Video- Hadley Wickham (2019): Data Visualization and Data Science\nName the technologies/techniques Wickham introduced. What are his main points? Summarize and comment.\nAfter watching this video, I was thoroughly impressed with the advancement of R Studio. Data visualization on R Studio would be so much more difficult and convoluted without the creation of the tidyverse package. This we have Hadley Wickham to thank for. Wickham is the creator of ggplot, ggplot2, and dplyr, tidyr, and purrr functions, all of which make up the package “tidyverse”. Below is a model that Wickham presented to visualize how different functions/packages are used in R Studio:\n\n\n\n\n\nI really appreciated Wickham’s relationship of his lecture to the infamous data set “gapminder”, which is a data set describing how countries’ incomes (GDP per capita), life expectancies, and populations changed over time. They also grouped the countries by region: the Americas, Europe, Asia, and Africa. Below is the code that he used to describe the components of ggplot2 code:\n\n\n\n\n\nWickham emphasizes the importance of learning the language of code, so that it will become easier to construct data visualization code in R Studio. He asserts that like linguistic languages, coding is also a language that needs to be deconstructed to become fluent. The code above therefore can be read as if it says, “Read the ‘gapminder’ dataset by filtering for just data from the year 2015, and create a new variable called ‘gapminder15’”.\nHe continues to discuss the power of ‘orthogonal components’. According to Wickham (2019), orthogonal components refer to the orthogonality of a data set, or the separability of different features within a system. Wickharm asserts that we must look at data as a function, and therefore like a function, data has distinct components or variables within it. Orthogonality therefore highlights specific variables of interest or pertinence to visualize and analyze. In the ‘gapminder’ set, Wickham highlights the concept of orthogonality by generating a plot with income/GDP per capita (on the x axis) and life expectancy (on the y axis) by country and region using the tidyverse package. Below is the generated chart.\n\n\n\n\n\nTo conclude, Wickham discusses the importance and the power of code. He highlights three main advantages to code: code is text, readable, and reproducible. Because code is text, it is able to be copied and pasted. Code is also a language, which means that other people can read it as well and possibly critique and even improve your code. Finally, code is reproducible, and allows you to rerun the code with updated or changed data easily and efficiently.\nAll of these points that Wickham makes further emphasize how helpful coding is to visualize and subsequently analyze data, especially with the help of developed code functions and packages. For this, we really have Wickham to thank!"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "I hope that Dr. Ho sees this and is proud."
  },
  {
    "objectID": "posts/assign01/index.html",
    "href": "posts/assign01/index.html",
    "title": "EPPS 6356 Assignment 1",
    "section": "",
    "text": "Google “generative art”. Cite some examples.\n\n\n\n\n\n\nHoff, Anders. 2016. Differential Lattice. https://inconvergent.net/generative/differential-lattice/ (September 13, 2022).\n\n\n\n\n\nStock, Mark J. 2021. Diaspora. http://markjstock.com/#/particle/ (September 13, 2022).\n\n\n\n\n\nBrunner, Katharina. 2022. When Two Points on a Circle Form a Line. https://katharinabrunner.de/generativeart/ (September 13, 2022).\n2. Run Fall.R (on class GitHub under R.\na. Give your own colors (e.g. Spring).\nb. Export the file and post on your GitHub website.\n\n\n\n\n\n3. Write a critique on a chart in published work (book/article/news website).\n\n\n\n\n\nMcGill, Kathryn A. and Stefurak, Tres. 2021. ‘“Man Up”: Sex-Differentiated Pathways of Juvenile Delinquency through Trauma, Borderline Traits & Offense Patterns’. Juvenile and Family Court Journal 72(3): 37-65.\nThe figure, Figure 3, is titled “Graph of Interaction Effect between Sex and Borderline Traits Predicting Status Offenses”. Interaction effects plots are used to visualize how the effect of one variable varies according to the value of another variable. According to McGill and Stefurak (2021), the female participants “exhibited higher rates of borderline traits, trauma symptoms and status offenses, as hypothesized, the sex by offending interaction suggests that these symptoms are unique predictors of male’s status offending, rather than for females”. This can be seen by the positive slope of the red line for male offenders in relation to borderline traits (Efforts to avoid real or imagined abandonment, a pattern of intense and unstable relationships, distorted and unstable self-image, including feelings of dissociation, e.g. feeling cut off from oneself, feeling outside of one’s body, impulsive and often dangerous behaviors, recurring thoughts of suicidal behaviors or threats of suicide or self-harming behaviors, e.g. cutting, highly unstable and changeable mood states, chronic feelings of emptiness, anger management difficulties, interpersonal distrust) and status offenses (McGill and Stefurak 2021). In contrast, negative slope of the green line for female offenders in relation to borderline traits and status offenses indicates these symptoms are not unique predictors of female’s status offending. Although the chart does clearly and evidently show the opposite effects sex has on the relationship between borderline traits and status offending, there are some important components of this chart that are missing. For instance, the chart does not have labels on any component of the figure. For this reason, there is no reference for the interpreter to understand what the measurements are for the x and y axes. Furthermore, there is no indication as to the significance of the x and y axes units or increments. The chart also has no figure description attached to it within the article, so the reader does not have anything to refer to while interpreting the chart. For this reason, it is difficult for the reader to understand the importance or pertinence of the figure alone or independently without referring to the discussion and conclusion of the article."
  },
  {
    "objectID": "blogindex.html",
    "href": "blogindex.html",
    "title": "samantha-manuel.github.io-blog",
    "section": "",
    "text": "EPPS6356\n\n\nAssignments\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2022\n\n\nSamantha Manuel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nEPPS6356\n\n\nReview\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2022\n\n\nSamantha Manuel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nEPPS6356\n\n\nAssignments\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2022\n\n\nSamantha Manuel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nEPPS6356\n\n\nReview\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2022\n\n\nSamantha Manuel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nEPPS6356\n\n\nAssignments\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2022\n\n\nSamantha Manuel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nEPPS6356\n\n\nEPPS6302\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2022\n\n\nSamantha Manuel\n\n\n\n\n\n\nNo matching items"
  }
]